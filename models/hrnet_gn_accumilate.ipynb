{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e25c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, ReLU, UpSampling2D, Lambda\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow_addons.layers import GroupNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f61c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(out_planes, stride=1):\n",
    "    \"\"\"3 x 3 convolution with padding\"\"\"\n",
    "    return Conv2D(filters=out_planes, kernel_size=3, strides=stride, padding=\"same\", use_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aef0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(tf.keras.layers.Layer):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, _in_channel, output_dim, stride=1, downsample=None, GN_GROUPS=32, **kwargs):\n",
    "        super(BasicBlock, self).__init__(**kwargs)\n",
    "        self.downsample = downsample\n",
    "        self.conv1 = conv3x3(output_dim, stride)\n",
    "        self.bn1 = GroupNormalization(groups=GN_GROUPS)\n",
    "        self.relu = ReLU()\n",
    "        self.conv2 = conv3x3(output_dim)\n",
    "        self.bn2 = GroupNormalization(groups=GN_GROUPS)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        residual = inputs\n",
    "\n",
    "        out = self.conv1(inputs)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(inputs)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6846b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(tf.keras.layers.Layer):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, _in_channel, output_dim, stride=1, downsample=None, GN_GROUPS=32, **kwargs):\n",
    "        super(Bottleneck, self).__init__(**kwargs)\n",
    "        self.downsample = downsample\n",
    "        self.conv1 = Conv2D(filters=output_dim, kernel_size=1, padding=\"same\", use_bias=False)\n",
    "        self.bn1 = GroupNormalization(groups=GN_GROUPS)\n",
    "        self.conv2 = Conv2D(filters=output_dim, kernel_size=3, strides=stride, padding=\"same\", use_bias=False)\n",
    "        self.bn2 = GroupNormalization(groups=GN_GROUPS)\n",
    "        self.conv3 = Conv2D(filters=output_dim*self.expansion, kernel_size=1, padding=\"same\", use_bias=False)\n",
    "        self.bn3 = GroupNormalization(groups=GN_GROUPS)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        residual = inputs\n",
    "\n",
    "        out = self.conv1(inputs)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(inputs)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9c8038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighResolutionModule(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_branches, blocks, num_blocks, num_inchannels, num_channels, GN_GROUPS, multi_scale_output=True, **kwargs):\n",
    "        super(HighResolutionModule, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_inchannels = num_inchannels\n",
    "        self.num_branches = num_branches\n",
    "        self.multi_scale_output = multi_scale_output\n",
    "        self.blocks = blocks\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_channels = num_channels\n",
    "        self.GN_GROUPS = GN_GROUPS\n",
    "\n",
    "        self.branches = self._make_branches(self.num_branches, self.blocks, self.num_blocks, self.num_channels)\n",
    "        self.fuse_layers = self._make_fuse_layers()\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def _make_one_branch(self, branch_index, block, num_blocks, num_channels, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n",
    "            downsample = Sequential(\n",
    "                Conv2D(\n",
    "                    self.num_inchannels[branch_index],\n",
    "                    num_channels[branch_index] * block.expansion,\n",
    "                    kernel_size=1, strides=stride, bias=False),\n",
    "                GroupNormalization(groups=self.GN_GROUPS),\n",
    "            )\n",
    "            \n",
    "        layers = [block(self.num_inchannels[branch_index], num_channels[branch_index], \n",
    "                        stride, downsample, GN_GROUPS=self.GN_GROUPS)]\n",
    "        self.num_inchannels[branch_index] = num_channels[branch_index] * block.expansion\n",
    "        for i in range(1, num_blocks[branch_index]):\n",
    "            layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index], \n",
    "                                GN_GROUPS=self.GN_GROUPS))\n",
    "        \n",
    "        return Sequential(layers)\n",
    "\n",
    "    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n",
    "        branches = []\n",
    "        for i in range(num_branches):\n",
    "            branches.append(self._make_one_branch(i, block, num_blocks, num_channels))\n",
    "        return branches\n",
    "\n",
    "    def _make_fuse_layers(self):\n",
    "        if self.num_branches == 1:\n",
    "            return None\n",
    "\n",
    "        num_branches = self.num_branches\n",
    "        num_inchannels = self.num_inchannels\n",
    "        fuse_layers = []\n",
    "        for i in range(num_branches if self.multi_scale_output else 1):\n",
    "            fuse_layer = []\n",
    "            for j in range(num_branches):\n",
    "                if j > i:\n",
    "                    fuse_layer.append(Sequential([\n",
    "                        Conv2D(\n",
    "                            num_inchannels[i], \n",
    "                            kernel_size=(1, 1), \n",
    "                            strides=(1, 1), \n",
    "                            padding=\"same\", \n",
    "                            use_bias=False),\n",
    "                        GroupNormalization(groups=self.GN_GROUPS),\n",
    "                        UpSampling2D(size=(2 ** (j - i), 2 ** (j - i)), interpolation='bilinear')\n",
    "                    ]))\n",
    "                elif j == i:\n",
    "                    fuse_layer.append(None)\n",
    "                else:\n",
    "                    conv3x3s = []\n",
    "                    for k in range(i - j):\n",
    "                        if k == i - j - 1:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[i]\n",
    "                            conv3x3s.append(Sequential([\n",
    "                                Conv2D(\n",
    "                                    num_outchannels_conv3x3,\n",
    "                                    kernel_size=(3,3),\n",
    "                                    strides=(2,2),\n",
    "                                    padding=\"same\",\n",
    "                                    use_bias=False),\n",
    "                                GroupNormalization(groups=self.GN_GROUPS),\n",
    "                            ]))\n",
    "                        else:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[j]\n",
    "                            conv3x3s.append(Sequential([\n",
    "                                Conv2D(\n",
    "                                    num_outchannels_conv3x3,\n",
    "                                    kernel_size=(3,3),\n",
    "                                    strides=(2,2),\n",
    "                                    padding=\"same\",\n",
    "                                    use_bias=False),\n",
    "                                GroupNormalization(groups=self.GN_GROUPS),\n",
    "                                ReLU()\n",
    "                            ]))\n",
    "                    fuse_layer.append(Sequential(conv3x3s))\n",
    "            fuse_layers.append(fuse_layer)\n",
    "\n",
    "        return fuse_layers\n",
    "\n",
    "    def get_num_inchannels(self):\n",
    "        return self.num_inchannels\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        if self.num_branches == 1:\n",
    "            return [self.branches[0](inputs[0])]\n",
    "\n",
    "        for i in range(self.num_branches):\n",
    "            inputs[i] = self.branches[i](inputs[i])\n",
    "\n",
    "        x_fuse = []\n",
    "\n",
    "        for i in range(len(self.fuse_layers)):\n",
    "            y = inputs[0] if i == 0 else self.fuse_layers[i][0](inputs[0])\n",
    "            # We cast to tf.float32 to ensure there is no mismatch between data types\n",
    "            # when using automatic mixed precision\n",
    "            y = tf.cast(y, tf.float32)\n",
    "            for j in range(1, self.num_branches):\n",
    "                if i == j:\n",
    "                    y = y + tf.cast(inputs[j], tf.float32)\n",
    "                elif j > 1:\n",
    "                    xx = self.fuse_layers[i][j](inputs[j])\n",
    "                    xx = tf.cast(xx, tf.float32)\n",
    "                    width_output = inputs[i].get_shape()[-2]\n",
    "                    height_output = inputs[i].get_shape()[-3]\n",
    "                    y = y + tf.image.resize(xx, size=(height_output, width_output))\n",
    "                else:\n",
    "                    xx = self.fuse_layers[i][j](inputs[j])\n",
    "                    xx = tf.cast(xx, tf.float32)\n",
    "                    y = y + xx\n",
    "            x_fuse.append(self.relu(y))\n",
    "        return x_fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f648b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_dict = {\n",
    "    'BASIC'     : BasicBlock,\n",
    "    'BOTTLENECK': Bottleneck\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d43db0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class HRNet_GN(tf.keras.models.Model):\n",
    "    def __init__(self, \n",
    "                 stage1_cfg, \n",
    "                 stage2_cfg, \n",
    "                 stage3_cfg, \n",
    "                 stage4_cfg, \n",
    "                 input_height, \n",
    "                 input_width, \n",
    "                 n_classes, \n",
    "                 W, \n",
    "                 GN_GROUPS,\n",
    "                 ACCUM_STEPS,\n",
    "                 *args, **kwargs):\n",
    "        super(HRNet_GN, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        C, C2, C4, C8 = W, int(W*2), int(W*4), int(W*8)\n",
    "        \n",
    "        stage1_cfg['NUM_CHANNELS'] = [C]\n",
    "        stage2_cfg['NUM_CHANNELS'] = [C, C2]\n",
    "        stage3_cfg['NUM_CHANNELS'] = [C, C2, C4]\n",
    "        stage4_cfg['NUM_CHANNELS'] = [C, C2, C4, C8]\n",
    "        \n",
    "        self.stage1_cfg = stage1_cfg\n",
    "        self.stage2_cfg = stage2_cfg\n",
    "        self.stage3_cfg = stage3_cfg\n",
    "        self.stage4_cfg = stage4_cfg\n",
    "        self.NUM_CLASSES = n_classes\n",
    "        self.GN_GROUPS = GN_GROUPS\n",
    "        self.ACCUM_STEPS = ACCUM_STEPS\n",
    "        self.inplanes = 64\n",
    "        self.input_height = input_height\n",
    "        self.input_width = input_width\n",
    "        self.W = W\n",
    "        \n",
    "        # stem net\n",
    "        self.conv1 = Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\", use_bias=False)\n",
    "        self.bn1 = GroupNormalization(groups=32)\n",
    "        self.conv2 = Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\", use_bias=False)\n",
    "        self.bn2 = GroupNormalization(groups=32)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "        # STAGE 1\n",
    "        num_channels = self.stage1_cfg['NUM_CHANNELS'][0]\n",
    "        block = blocks_dict[self.stage1_cfg['BLOCK']]\n",
    "        num_blocks = self.stage1_cfg['NUM_BLOCKS'][0]\n",
    "        self.layer1 = self._make_layer(block, num_channels, num_blocks)\n",
    "        stage1_out_channel = block.expansion * num_channels\n",
    "\n",
    "        # STAGE 2\n",
    "        num_channels = self.stage2_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage2_cfg['BLOCK']]\n",
    "        num_channels = [num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition1 = self._make_transition_layer([stage1_out_channel], num_channels, GN_GROUPS=self.GN_GROUPS)\n",
    "        self.stage2, pre_stage_channels = self._make_stage(self.stage2_cfg, num_channels, GN_GROUPS=self.GN_GROUPS)\n",
    "\n",
    "        # STAGE 3\n",
    "        num_channels = self.stage3_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage3_cfg['BLOCK']]\n",
    "        num_channels = [num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition2 = self._make_transition_layer(pre_stage_channels, num_channels, GN_GROUPS=self.GN_GROUPS)\n",
    "        self.stage3, pre_stage_channels = self._make_stage(self.stage3_cfg, num_channels, GN_GROUPS=self.GN_GROUPS)\n",
    "\n",
    "        # STAGE 4\n",
    "        num_channels = self.stage4_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage4_cfg['BLOCK']]\n",
    "        num_channels = [num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition3 = self._make_transition_layer(pre_stage_channels, num_channels, GN_GROUPS=self.GN_GROUPS)\n",
    "        self.stage4, pre_stage_channels = self._make_stage(self.stage4_cfg, num_channels, GN_GROUPS=self.GN_GROUPS)\n",
    "\n",
    "        last_inp_channels = np.int(np.sum(pre_stage_channels))\n",
    "        \n",
    "        # Upsample\n",
    "        self.upsample_out = Lambda(lambda xx: tf.image.resize(xx, size=(input_height, input_width)))\n",
    "\n",
    "        # Last layer\n",
    "        self.last_layer = Sequential([\n",
    "            Conv2D(filters=last_inp_channels, kernel_size=1, strides=1, padding=\"same\", use_bias=False),\n",
    "            GroupNormalization(groups=self.GN_GROUPS),\n",
    "            ReLU(),\n",
    "            Conv2D(filters=self.NUM_CLASSES, kernel_size=1, strides=1, padding=\"same\", dtype=\"float32\")\n",
    "        ])\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _make_transition_layer(num_channels_pre_layer, num_channels_cur_layer, GN_GROUPS):\n",
    "        num_branches_pre = len(num_channels_pre_layer)\n",
    "        num_branches_cur = len(num_channels_cur_layer)\n",
    "\n",
    "        transition_layers = []\n",
    "        for i in range(num_branches_cur):\n",
    "            if i < num_branches_pre:\n",
    "                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n",
    "                    transition_layers.append(Sequential([\n",
    "                        Conv2D(\n",
    "                            filters=num_channels_cur_layer[i],\n",
    "                            kernel_size=(3,3),\n",
    "                            strides=(1,1),\n",
    "                            padding=\"same\",\n",
    "                            use_bias=False),\n",
    "                        GroupNormalization(groups=GN_GROUPS),\n",
    "                        ReLU()\n",
    "                    ]))\n",
    "                else:\n",
    "                    transition_layers.append(None)\n",
    "            else:\n",
    "                conv3x3s = []\n",
    "                for j in range(i + 1 - num_branches_pre):\n",
    "                    inchannels = num_channels_pre_layer[-1]\n",
    "                    outchannels = num_channels_cur_layer[i] if j == i - num_branches_pre else inchannels\n",
    "                    conv3x3s.append(Sequential([\n",
    "                        Conv2D(\n",
    "                            filters=outchannels, \n",
    "                            kernel_size=(3,3), \n",
    "                            strides=(2,2), \n",
    "                            padding=\"same\", \n",
    "                            use_bias=False\n",
    "                        ),\n",
    "                        GroupNormalization(groups=GN_GROUPS),\n",
    "                        ReLU()\n",
    "                    ]))\n",
    "                transition_layers.append(Sequential(conv3x3s))\n",
    "\n",
    "        return transition_layers\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_stage(layer_config, num_inchannels, GN_GROUPS, multi_scale_output=True):\n",
    "        num_modules = layer_config['NUM_MODULES']\n",
    "        num_branches = layer_config['NUM_BRANCHES']\n",
    "        num_channels = layer_config['NUM_CHANNELS']\n",
    "        num_blocks = layer_config['NUM_BLOCKS']\n",
    "        block = blocks_dict[layer_config['BLOCK']]\n",
    "        \n",
    "        modules = []\n",
    "        for i in range(num_modules):\n",
    "\n",
    "            if not multi_scale_output and i == num_modules - 1:\n",
    "                reset_multi_scale_output = False\n",
    "            else:\n",
    "                reset_multi_scale_output = True\n",
    "\n",
    "            modules.append(\n",
    "                HighResolutionModule(\n",
    "                    num_branches=num_branches,\n",
    "                    blocks=block,\n",
    "                    num_blocks=num_blocks,\n",
    "                    num_inchannels=num_inchannels,\n",
    "                    num_channels=num_channels,\n",
    "                    multi_scale_output=reset_multi_scale_output,\n",
    "                    GN_GROUPS=GN_GROUPS\n",
    "                )\n",
    "            )\n",
    "            num_inchannels = modules[-1].get_num_inchannels()\n",
    "\n",
    "        return modules, num_inchannels\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = Sequential([\n",
    "                Conv2D(\n",
    "                    planes * block.expansion, \n",
    "                    kernel_size=(1,1), \n",
    "                    strides=(stride,stride), \n",
    "                    padding=\"same\",use_bias=False\n",
    "                ),\n",
    "                GroupNormalization(groups=self.GN_GROUPS)\n",
    "            ])\n",
    "\n",
    "        layers = [block(self.inplanes, planes, stride, downsample, GN_GROUPS=self.GN_GROUPS)]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, GN_GROUPS=self.GN_GROUPS))\n",
    "\n",
    "        return Sequential(layers)\n",
    "\n",
    "    @staticmethod\n",
    "    def _forward_stage(stage, xs):\n",
    "        ys = xs\n",
    "\n",
    "        for module in stage:\n",
    "            ys = module(ys)\n",
    "            if not isinstance(ys, list):\n",
    "                ys = [ys]\n",
    "        return ys\n",
    "\n",
    "    \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # STAGE 1\n",
    "        x = self.layer1(x)\n",
    "\n",
    "        # STAGE 2\n",
    "        x_list = []\n",
    "        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n",
    "            if self.transition1[i] is not None:\n",
    "                x_list.append(self.transition1[i](x))\n",
    "            else:\n",
    "                x_list.append(x)\n",
    "        y_list = self._forward_stage(self.stage2, x_list)\n",
    "\n",
    "        # STAGE 3\n",
    "        x_list = []\n",
    "        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n",
    "            if self.transition2[i] is not None:\n",
    "                x_list.append(self.transition2[i](y_list[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        y_list = self._forward_stage(self.stage3, x_list)\n",
    "\n",
    "        # STAGE 4\n",
    "        x_list = []\n",
    "        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n",
    "            if self.transition3[i] is not None:\n",
    "                x_list.append(self.transition3[i](y_list[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        x = self._forward_stage(self.stage4, x_list)\n",
    "\n",
    "        # Upsampling\n",
    "        x0 = tf.cast(x[0], tf.float32)\n",
    "        x0 = self.upsample_out(x0)\n",
    "        x1 = self.upsample_out(x[1])\n",
    "        x2 = self.upsample_out(x[2])\n",
    "        x3 = self.upsample_out(x[3])\n",
    "        \n",
    "        # Concat\n",
    "        x = tf.concat([x0, x1, x2, x3], axis=3)\n",
    "\n",
    "        x = self.last_layer(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def train_step(self, data):\n",
    "        self.n_acum_step.assign_add(1)\n",
    "\n",
    "        x, y = data\n",
    "        # Gradient Tape\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "            \n",
    "        # Calculate batch gradients\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "            \n",
    "        # Accumulate batch gradients\n",
    "        for i in range(len(self.gradient_accumulation)):\n",
    "            self.gradient_accumulation[i].assign_add(gradients[i])\n",
    " \n",
    "        # If n_acum_step reach the n_gradients then we apply accumulated gradients to update the variables \n",
    "        # otherwise do nothing\n",
    "        tf.cond(tf.equal(self.n_acum_step, self.n_gradients), self.apply_accu_gradients, lambda: None)\n",
    "\n",
    "        # update metrics\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    \n",
    "    def apply_accu_gradients(self):\n",
    "        # apply accumulated gradients\n",
    "        self.optimizer.apply_gradients(zip(self.gradient_accumulation, self.trainable_variables))\n",
    "\n",
    "        # reset\n",
    "        self.n_acum_step.assign(0)\n",
    "        for i in range(len(self.gradient_accumulation)):\n",
    "            self.gradient_accumulation[i].assign(tf.zeros_like(self.trainable_variables[i], dtype=tf.float32))\n",
    "            \n",
    "            \n",
    "    def build_model(self):\n",
    "            \n",
    "        # Initialize weights of the network\n",
    "        inp_test = tf.random.normal(shape=(1, self.input_height, self.input_width, 3))\n",
    "        out_test = self(inp_test)\n",
    "\n",
    "        self._name = \"HRNet_GN_W{}\".format(self.W)\n",
    "        \n",
    "        \n",
    "         # Gradient accumilation\n",
    "        self.n_gradients = tf.constant(self.ACCUM_STEPS, dtype=tf.int32)\n",
    "        self.n_acum_step = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
    "        self.gradient_accumulation = [tf.Variable(tf.zeros_like(v, dtype=tf.float32), \n",
    "                                                  trainable=False) for v in self.trainable_variables]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
